{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import join, splitext, basename\n",
    "import glob\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as trans\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import Sampler\n",
    "from PIL import Image\n",
    "from imgaug import augmenters as iaa\n",
    "from matplotlib.pyplot import figure, imshow, axis\n",
    "import imgaug as ia\n",
    "import numpy as np\n",
    "import PIL\n",
    "import torch\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import statistics\n",
    "import random\n",
    "import natsort\n",
    "import copy\n",
    "import collections\n",
    "import torchvision.models as models\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Check devices..\n",
      "Current device:  cuda\n",
      "Our selected device:  0\n",
      "1  GPUs is available\n"
     ]
    }
   ],
   "source": [
    "#To determine if your system supports CUDA\n",
    "print(\"==> Check devices..\")\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"Current device: \",device)\n",
    "\n",
    "#Also can print your current GPU id, and the number of GPUs you can use.\n",
    "print(\"Our selected device: \", torch.cuda.current_device())\n",
    "print(torch.cuda.device_count(), \" GPUs is available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config():\n",
    "    def __init__(self):\n",
    "        self.FolderNames2English_names = {\n",
    "                                    '0':\"Bread\",          # 994 \n",
    "                                    '1':\"Dairy_product\",  # 429\n",
    "                                    '2':\"Dessert\",        # 1500\n",
    "                                    '3':\"Egg\",            # 986\n",
    "                                    '4':\"Fried_food\",     # 848\n",
    "                                    '5':\"Meat\",           # 1325\n",
    "                                    '6':\"Noodles\",        # 440\n",
    "                                    '7':\"Rice\",           # 280\n",
    "                                    '8':\"Seafood\",        # 855\n",
    "                                    '9':\"Soup\",           # 1500\n",
    "                                    '10':\"Vegetable_fruit\"# 709\n",
    "                                    }\n",
    "        self.folder_names2code = {}\n",
    "        self.image_size = 224\n",
    "        self.early_stop = 8\n",
    "        self.max_epoch = 1000\n",
    "        self.train_batchsize = 128\n",
    "        self.eva_val_batchsize = 32\n",
    "        self.class_num = 11\n",
    "        self.each_class_item_num = {}\n",
    "        self.temperature = 1\n",
    "        self.alpha = 0.5\n",
    "        \n",
    "        \n",
    "        self.train_dataset_path = r'./food11re/training'\n",
    "        self.validation_dataset_path = r'./food11re/validation'\n",
    "        self.test_dataset_path = r'./food11re/evaluation'\n",
    "        self.model_ouput_dir = './model/'\n",
    "        self.teacher_model_path = './teacher_model/7.pth'\n",
    "        self.best_epoch = 0\n",
    "        class_folder_name = listdir(self.test_dataset_path)\n",
    "        self.class_folder_num = {}\n",
    "        for cf in class_folder_name:\n",
    "            self.class_folder_num[cf] = len(listdir(self.test_dataset_path + '/' + cf))\n",
    "            \n",
    "        \n",
    "        self.net = 'resnet18'  # 0: resnet18\n",
    "        self.teacher_net = 'resnet101'\n",
    "        self.pretrain = False\n",
    "\n",
    "        self.wts = [500/self.class_folder_num['0'], 500/self.class_folder_num['1'], 500/self.class_folder_num['2'], \n",
    "                    500/self.class_folder_num['3'], 500/self.class_folder_num['4'], 500/self.class_folder_num['5'], \n",
    "                    500/self.class_folder_num['6'], 500/self.class_folder_num['7'], 500/self.class_folder_num['8'], \n",
    "                    500/self.class_folder_num['9'], 500/self.class_folder_num['10']]\n",
    "        self.lr = 0.0001\n",
    "        self.criterion = nn.CrossEntropyLoss() #定義損失函數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_image_file(filename):\n",
    "    return any(filename.endswith(extension) for extension in [\".png\", \".jpg\", \".jpeg\"])\n",
    "\n",
    "def load_img(filepath):\n",
    "    img = Image.open(filepath)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImgAugTransform():\n",
    "    def __init__(self, config=Config()):\n",
    "        self.aug = iaa.Sequential([\n",
    "            iaa.Scale((config.image_size, config.image_size)),\n",
    "            iaa.Sometimes(0.25, iaa.GaussianBlur(sigma=(0, 3.0))),\n",
    "            iaa.Fliplr(0.5),\n",
    "            iaa.Affine(rotate=(-20, 20), mode='symmetric'),\n",
    "            iaa.Sometimes(0.25,\n",
    "                      iaa.OneOf([iaa.Dropout(p=(0, 0.1)),\n",
    "                                 iaa.CoarseDropout(0.1, size_percent=0.5)])),  # 對batch中的一部分圖片應用一部分Augmenters,剩下的圖片應用另外的Augmenters。\n",
    "            iaa.AddToHueAndSaturation(value=(-10, 10), per_channel=True)  # 即修改色調和飽和度\n",
    "        ])\n",
    "      \n",
    "    def __call__(self, img):\n",
    "        img = np.array(img)\n",
    "        return self.aug.augment_image(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WRSampler(dataset, wts):\n",
    "    class_name_list = dataset.classes\n",
    "    num_per_classes = {}\n",
    "    for img in dataset.imgs:\n",
    "        if  img[1] not in num_per_classes:\n",
    "            num_per_classes[int(img[1])] = 1\n",
    "        else:\n",
    "            num_per_classes[int(img[1])] += 1\n",
    "            \n",
    "    each_data_wts = []\n",
    "    for class_name in class_name_list:\n",
    "        class_item_num = num_per_classes[int(class_name)]\n",
    "        for i in range(class_item_num):\n",
    "            each_data_wts.append(wts[int(class_name)])\n",
    "    \n",
    "    sampler = torch.utils.data.sampler.WeightedRandomSampler(each_data_wts, len(each_data_wts), replacement=True)\n",
    "    \n",
    "    return sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, criterion, optimizer, max_epoch, train_loader, validation_loader, config):\n",
    "    t_loss = []\n",
    "    v_loss = []\n",
    "    training_accuracy = []\n",
    "    validation_accuracy = []\n",
    "    total = 0\n",
    "    min_val_loss = 0.0\n",
    "    min_val_error = 0.0\n",
    "    early_stop_timer = 0 \n",
    "\n",
    "    for epoch in range(max_epoch):  # loop over the dataset multiple times\n",
    "        train_loss = 0.0\n",
    "        validation_loss = 0.0\n",
    "        correct_train = 0\n",
    "        correct_validation = 0\n",
    "        train_num = 0\n",
    "        val_num = 0\n",
    "        train_img_num = 0\n",
    "        validation_img_num = 0\n",
    "\n",
    "\n",
    "        ########################\n",
    "        # train the model      #\n",
    "        ########################\n",
    "\n",
    "        model.train()\n",
    "        for i, (inputs, labels) in enumerate(train_loader, 0):\n",
    "\n",
    "            #change the type into cuda tensor \n",
    "            inputs = inputs.to(device) \n",
    "            labels = labels.to(device) \n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(inputs)\n",
    "            # select the class with highest probability\n",
    "            _, pred = outputs.max(1)\n",
    "            # if the model predicts the same results as the true\n",
    "            # label, then the correct counter will plus 1\n",
    "            correct_train += pred.eq(labels).sum().item()\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            train_loss += loss.item()\n",
    "            train_num += 1\n",
    "            train_img_num += len(labels)\n",
    "\n",
    "\n",
    "        ########################\n",
    "        # validate the model   #\n",
    "        ########################\n",
    "\n",
    "        model.eval()\n",
    "        for i, (inputs, labels) in enumerate(validation_loader, 0):\n",
    "            # move tensors to GPU if CUDA is available\n",
    "            inputs = inputs.to(device) \n",
    "            labels = labels.to(device)\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            outputs = model(inputs)\n",
    "            _, pred = outputs.max(1)\n",
    "            correct_validation += pred.eq(labels).sum().item()\n",
    "            # calculate the batch loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            # update average validation loss \n",
    "            validation_loss += loss.item()\n",
    "            val_num += 1\n",
    "            validation_img_num += len(labels)\n",
    "\n",
    "\n",
    "        if epoch % 1 == 0:    # print every 200 mini-batches\n",
    "            val_error = 1 - correct_validation / validation_img_num\n",
    "            print('[%d, %5d] train_loss: %.3f' % (epoch, max_epoch, train_loss / train_num))\n",
    "            print('[%d, %5d] validation_loss: %.3f' % (epoch, max_epoch, validation_loss / val_num))\n",
    "            print('%d epoch, training accuracy: %.4f' % (epoch, correct_train / train_img_num))\n",
    "            print('%d epoch, validation accuracy: %.4f' % (epoch, correct_validation / validation_img_num))\n",
    "\n",
    "\n",
    "            if epoch == 0:\n",
    "                min_val_error = val_error\n",
    "                print('Current best.')\n",
    "\n",
    "            if val_error < min_val_error:\n",
    "                min_val_error = val_error\n",
    "                config.best_epoch = epoch\n",
    "                early_stop_timer = 0\n",
    "                print('Current best.')\n",
    "            else:\n",
    "                early_stop_timer += 1\n",
    "                if early_stop_timer >= config.early_stop:\n",
    "                    print('Early Stop.\\nBest epoch is', str(config.best_epoch))\n",
    "                    break\n",
    "            t_loss.append(train_loss / train_num)\n",
    "            training_accuracy.append(correct_train / train_img_num)\n",
    "            validation_accuracy.append(correct_validation / validation_img_num)\n",
    "            running_loss = 0.0\n",
    "            validation_loss = 0.0\n",
    "            train_num = 0\n",
    "            val_num = 0\n",
    "            correct_train = 0\n",
    "            correct_validation = 0\n",
    "            total = 0\n",
    "            print('-----------------------------------------')\n",
    "\n",
    "            torch.save(model.state_dict(), config.model_ouput_dir + str(epoch) + '.pth')\n",
    "\n",
    "    print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_teacher_outputs(teacher_model, inputs):\n",
    "    teacher_model.eval()   \n",
    "    teacher_outputs = teacher_model(inputs)\n",
    "    \n",
    "    return teacher_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn_kd(outputs, labels, teacher_outputs, config):\n",
    "    \"\"\"\n",
    "    outputs        : training prediction of inputs.\n",
    "    labels         : hard labels.\n",
    "    teacher_outputs: soft labels.\n",
    "    config         : config including alpha and temperature.\n",
    "    \"\"\"\n",
    "    \n",
    "    alpha = config.alpha\n",
    "    temperature = config.temperature\n",
    "    \n",
    "    KD_loss = nn.KLDivLoss()(nn.functional.log_softmax(outputs/temperature, dim=1),\n",
    "                             nn.functional.softmax(teacher_outputs/temperature, dim=1)) * (alpha * temperature * temperature) + \\\n",
    "                             nn.functional.cross_entropy(outputs, labels) * (1. - alpha)\n",
    "\n",
    "    return KD_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainKD(model, teacher_model, criterion, optimizer, max_epoch, train_loader, validation_loader, config):\n",
    "    t_loss = []\n",
    "    v_loss = []\n",
    "    training_accuracy = []\n",
    "    validation_accuracy = []\n",
    "    total = 0\n",
    "    min_val_loss = 0.0\n",
    "    min_val_error = 0.0\n",
    "    early_stop_timer = 0 \n",
    "\n",
    "\n",
    "    for epoch in range(max_epoch):  # loop over the dataset multiple times\n",
    "        train_loss = 0.0\n",
    "        validation_loss = 0.0\n",
    "        correct_train = 0\n",
    "        correct_validation = 0\n",
    "        train_num = 0\n",
    "        val_num = 0\n",
    "        train_img_num = 0\n",
    "        validation_img_num = 0\n",
    "\n",
    "\n",
    "        ########################\n",
    "        # train the model      #\n",
    "        ########################\n",
    "\n",
    "        model.train()\n",
    "        for i, (inputs, labels) in enumerate(train_loader, 0):\n",
    "\n",
    "            #change the type into cuda tensor \n",
    "            inputs = inputs.to(device) \n",
    "            labels = labels.to(device) \n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(inputs)\n",
    "            teacher_outputs = get_teacher_outputs(teacher_model, inputs)\n",
    "            \n",
    "            _, pred = outputs.max(1)\n",
    "            # if the model predicts the same results as the true\n",
    "            # label, then the correct counter will plus 1\n",
    "            correct_train += pred.eq(labels).sum().item()\n",
    "            loss = loss_fn_kd(outputs, labels, teacher_outputs, config)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            train_loss += loss.item()\n",
    "            train_num += 1\n",
    "            train_img_num += len(labels)\n",
    "\n",
    "\n",
    "        ########################\n",
    "        # validate the model   #\n",
    "        ########################\n",
    "\n",
    "        model.eval()\n",
    "        for i, (inputs, labels) in enumerate(validation_loader, 0):\n",
    "            # move tensors to GPU if CUDA is available\n",
    "            inputs = inputs.to(device) \n",
    "            labels = labels.to(device)\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            outputs = model(inputs)\n",
    "            _, pred = outputs.max(1)\n",
    "            correct_validation += pred.eq(labels).sum().item()\n",
    "            # calculate the batch loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            # update average validation loss \n",
    "            validation_loss += loss.item()\n",
    "            val_num += 1\n",
    "            validation_img_num += len(labels)\n",
    "\n",
    "\n",
    "        if epoch % 1 == 0:    # print every 200 mini-batches\n",
    "            val_error = 1 - correct_validation / validation_img_num\n",
    "            print('[%d, %5d] train_loss: %.3f' % (epoch, max_epoch, train_loss / train_num))\n",
    "            print('[%d, %5d] validation_loss: %.3f' % (epoch, max_epoch, validation_loss / val_num))\n",
    "            print('%d epoch, training accuracy: %.4f' % (epoch, correct_train / train_img_num))\n",
    "            print('%d epoch, validation accuracy: %.4f' % (epoch, correct_validation / validation_img_num))\n",
    "\n",
    "\n",
    "            if epoch == 0:\n",
    "                min_val_error = val_error\n",
    "                print('Current best.')\n",
    "                \n",
    "            if val_error < min_val_error:\n",
    "                min_val_error = val_error\n",
    "                config.best_epoch = epoch\n",
    "                early_stop_timer = 0\n",
    "                print('Current best.')\n",
    "            else:\n",
    "                early_stop_timer += 1\n",
    "                if early_stop_timer >= config.early_stop:\n",
    "                    print('Early Stop.\\nBest epoch is', str(config.best_epoch))\n",
    "                    break\n",
    "            t_loss.append(train_loss / train_num)\n",
    "            training_accuracy.append(correct_train / train_img_num)\n",
    "            validation_accuracy.append(correct_validation / validation_img_num)\n",
    "            running_loss = 0.0\n",
    "            validation_loss = 0.0\n",
    "            train_num = 0\n",
    "            val_num = 0\n",
    "            correct_train = 0\n",
    "            correct_validation = 0\n",
    "            total = 0\n",
    "            print('-----------------------------------------')\n",
    "\n",
    "            torch.save(model.state_dict(), config.model_ouput_dir + str(epoch) + '.pth')\n",
    "\n",
    "    print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reload_net(config):\n",
    "    print('-----------------------------------------')\n",
    "    print('Reload', config.model_ouput_dir + '/' + str(config.best_epoch) + '.pth model.')\n",
    "    print('-----------------------------------------')\n",
    "    \n",
    "    if config.net == 'resnet18':\n",
    "        net = models.resnet18(pretrained=False)\n",
    "        net.fc = nn.Sequential(nn.Linear(512,256),nn.LeakyReLU(),nn.Linear(256,128),nn.LeakyReLU(),nn.Linear(128,config.class_num))\n",
    "        net.load_state_dict(torch.load(config.model_ouput_dir + '/' + str(config.best_epoch) + '.pth'))\n",
    "    elif config.net == 'densenet121':\n",
    "        net = models.densenet121(pretrained=False)\n",
    "        net.fc = nn.Sequential(nn.Linear(1000,256),nn.LeakyReLU(),nn.Linear(256,128),nn.LeakyReLU(),nn.Linear(128,config.class_num))\n",
    "        net.load_state_dict(torch.load(config.model_ouput_dir + '/' + str(config.best_epoch) + '.pth'))\n",
    "    elif config.net == 'resnet101':\n",
    "        net = models.resnet101(pretrained=False)\n",
    "        net.fc = nn.Sequential(nn.Linear(2048,256),nn.LeakyReLU(),nn.Linear(256,128),nn.LeakyReLU(),nn.Linear(128,config.class_num))\n",
    "        net.load_state_dict(torch.load(config.model_ouput_dir + '/' + str(config.best_epoch) + '.pth'))\n",
    "\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reload_teacher_net(config):\n",
    "    print('-----------------------------------------')\n",
    "    print('Reload', config.teacher_model_path, 'model.')\n",
    "    print('-----------------------------------------')\n",
    "    \n",
    "    if config.teacher_net == 'resnet18':\n",
    "        net = models.resnet18(pretrained=False)\n",
    "        net.fc = nn.Sequential(nn.Linear(512,256),nn.LeakyReLU(),nn.Linear(256,128),nn.LeakyReLU(),nn.Linear(128,config.class_num))\n",
    "        net.load_state_dict(torch.load(config.teacher_model_path))\n",
    "    elif config.teacher_net == 'densenet121':\n",
    "        net = models.densenet121(pretrained=False)\n",
    "        net.fc = nn.Sequential(nn.Linear(1000,256),nn.LeakyReLU(),nn.Linear(256,128),nn.LeakyReLU(),nn.Linear(128,config.class_num))\n",
    "        net.load_state_dict(torch.load(config.teacher_model_path))\n",
    "    elif config.teacher_net == 'resnet101':\n",
    "        net = models.resnet101(pretrained=False)\n",
    "        net.fc = nn.Sequential(nn.Linear(2048,256),nn.LeakyReLU(),nn.Linear(256,128),nn.LeakyReLU(),nn.Linear(128,config.class_num))\n",
    "        net.load_state_dict(torch.load(config.teacher_model_path))\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(model, evaluation_dataset, evaluation_loader, config):\n",
    "    test_loss = 0.0\n",
    "    correct_test = 0\n",
    "    test_num = 0\n",
    "    cls = np.zeros(config.class_num)\n",
    "    correct_top3 = 0\n",
    "    class_folder_num = config.class_folder_num\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    for i, (inputs, labels) in enumerate(evaluation_loader, 0):\n",
    "        # move tensors to GPU if CUDA is available\n",
    "        inputs = inputs.to(device) \n",
    "        labels = labels.to(device)\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        outputs = model(inputs)\n",
    "        _, pred = outputs.max(1)\n",
    "        correct_test += pred.eq(labels).sum().item()\n",
    "        _, top3 = outputs.topk(3)\n",
    "        correct_top3 += top3.eq(labels.view(-1,1).expand_as(top3)).sum().item()\n",
    "\n",
    "        for j in range(config.class_num):\n",
    "            cls[j] += (pred.eq(j) * pred.eq(labels)).sum().item()\n",
    "\n",
    "    print('Test set: Top 1 Accuracy: %d/%d (%.2f%%), Top 3 Accuracy: %d/%d (%.2f%%)' \n",
    "          % (correct_test, len(evaluation_dataset), correct_test / len(evaluation_dataset)*100, correct_top3, len(evaluation_dataset),\n",
    "             correct_top3/ len(evaluation_dataset)*100))\n",
    "\n",
    "    FN2EN = config.FolderNames2English_names\n",
    "\n",
    "    print('%-20s : %d/%d    %10f%%' % (FN2EN['0'], cls[config.folder_names2code['0']], class_folder_num['0'], cls[config.folder_names2code['0']]/class_folder_num['0']*100))                                                    \n",
    "    print('%-20s : %d/%d    %10f%%' % (FN2EN['1'], cls[config.folder_names2code['1']], class_folder_num['1'], cls[config.folder_names2code['1']]/class_folder_num['1']*100))\n",
    "    print('%-20s : %d/%d    %10f%%' % (FN2EN['2'], cls[config.folder_names2code['2']], class_folder_num['2'], cls[config.folder_names2code['2']]/class_folder_num['2']*100))\n",
    "    print('%-20s : %d/%d    %10f%%' % (FN2EN['3'], cls[config.folder_names2code['3']], class_folder_num['3'], cls[config.folder_names2code['3']]/class_folder_num['3']*100))\n",
    "    print('%-20s : %d/%d    %10f%%' % (FN2EN['4'], cls[config.folder_names2code['4']], class_folder_num['4'], cls[config.folder_names2code['4']]/class_folder_num['4']*100))\n",
    "    print('%-20s : %d/%d    %10f%%' % (FN2EN['5'], cls[config.folder_names2code['5']], class_folder_num['5'], cls[config.folder_names2code['5']]/class_folder_num['5']*100))\n",
    "    print('%-20s : %d/%d    %10f%%' % (FN2EN['6'], cls[config.folder_names2code['6']], class_folder_num['6'], cls[config.folder_names2code['6']]/class_folder_num['6']*100))\n",
    "    print('%-20s : %d/%d    %10f%%' % (FN2EN['7'], cls[config.folder_names2code['7']], class_folder_num['7'], cls[config.folder_names2code['7']]/class_folder_num['7']*100))\n",
    "    print('%-20s : %d/%d    %10f%%' % (FN2EN['8'], cls[config.folder_names2code['8']], class_folder_num['8'], cls[config.folder_names2code['8']]/class_folder_num['8']*100))\n",
    "    print('%-20s : %d/%d    %10f%%' % (FN2EN['9'], cls[config.folder_names2code['9']], class_folder_num['9'], cls[config.folder_names2code['9']]/class_folder_num['9']*100))\n",
    "    print('%-20s : %d/%d    %10f%%' % (FN2EN['10'], cls[config.folder_names2code['10']], class_folder_num['10'], cls[config.folder_names2code['10']]/class_folder_num['10']*100))\n",
    "\n",
    "\n",
    "    avg = []\n",
    "    avg.append(cls[config.folder_names2code['0']]/class_folder_num['0']*100)\n",
    "    avg.append(cls[config.folder_names2code['1']]/class_folder_num['1']*100)\n",
    "    avg.append(cls[config.folder_names2code['2']]/class_folder_num['2']*100)\n",
    "    avg.append(cls[config.folder_names2code['3']]/class_folder_num['3']*100)\n",
    "    avg.append(cls[config.folder_names2code['4']]/class_folder_num['4']*100)\n",
    "    avg.append(cls[config.folder_names2code['5']]/class_folder_num['5']*100)\n",
    "    avg.append(cls[config.folder_names2code['6']]/class_folder_num['6']*100)\n",
    "    avg.append(cls[config.folder_names2code['7']]/class_folder_num['7']*100)\n",
    "    avg.append(cls[config.folder_names2code['8']]/class_folder_num['8']*100)\n",
    "    avg.append(cls[config.folder_names2code['9']]/class_folder_num['9']*100)\n",
    "    avg.append(cls[config.folder_names2code['10']]/class_folder_num['10']*100)\n",
    "\n",
    "    print('Average per case accuracy: %10f%%' % (sum(avg)/len(avg)))\n",
    "    print('-----------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/weienv/lib/python3.7/site-packages/imgaug/imgaug.py:184: DeprecationWarning: Function `Scale()` is deprecated. Use `Resize` instead. Resize has the exactly same interface as Scale.\n",
      "  warn_deprecated(msg, stacklevel=3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "Reload ./teacher_model/7.pth model.\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/weienv/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  warnings.warn(\"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0,  1000] train_loss: 1.155\n",
      "[0,  1000] validation_loss: 1.982\n",
      "0 epoch, training accuracy: 0.2582\n",
      "0 epoch, validation accuracy: 0.2822\n",
      "Current best.\n",
      "-----------------------------------------\n",
      "[1,  1000] train_loss: 1.026\n",
      "[1,  1000] validation_loss: 1.873\n",
      "1 epoch, training accuracy: 0.3483\n",
      "1 epoch, validation accuracy: 0.3434\n",
      "Current best.\n",
      "-----------------------------------------\n",
      "[2,  1000] train_loss: 0.923\n",
      "[2,  1000] validation_loss: 1.716\n",
      "2 epoch, training accuracy: 0.4132\n",
      "2 epoch, validation accuracy: 0.4064\n",
      "Current best.\n",
      "-----------------------------------------\n",
      "[3,  1000] train_loss: 0.863\n",
      "[3,  1000] validation_loss: 1.632\n",
      "3 epoch, training accuracy: 0.4534\n",
      "3 epoch, validation accuracy: 0.4283\n",
      "Current best.\n",
      "-----------------------------------------\n",
      "[4,  1000] train_loss: 0.800\n",
      "[4,  1000] validation_loss: 1.595\n",
      "4 epoch, training accuracy: 0.4952\n",
      "4 epoch, validation accuracy: 0.4633\n",
      "Current best.\n",
      "-----------------------------------------\n",
      "[5,  1000] train_loss: 0.768\n",
      "[5,  1000] validation_loss: 1.615\n",
      "5 epoch, training accuracy: 0.5152\n",
      "5 epoch, validation accuracy: 0.4627\n",
      "-----------------------------------------\n",
      "[6,  1000] train_loss: 0.754\n",
      "[6,  1000] validation_loss: 1.503\n",
      "6 epoch, training accuracy: 0.5152\n",
      "6 epoch, validation accuracy: 0.4802\n",
      "Current best.\n",
      "-----------------------------------------\n",
      "[7,  1000] train_loss: 0.693\n",
      "[7,  1000] validation_loss: 1.564\n",
      "7 epoch, training accuracy: 0.5644\n",
      "7 epoch, validation accuracy: 0.4706\n",
      "-----------------------------------------\n",
      "[8,  1000] train_loss: 0.660\n",
      "[8,  1000] validation_loss: 1.723\n",
      "8 epoch, training accuracy: 0.5814\n",
      "8 epoch, validation accuracy: 0.4501\n",
      "-----------------------------------------\n",
      "[9,  1000] train_loss: 0.628\n",
      "[9,  1000] validation_loss: 1.430\n",
      "9 epoch, training accuracy: 0.6074\n",
      "9 epoch, validation accuracy: 0.5219\n",
      "Current best.\n",
      "-----------------------------------------\n",
      "[10,  1000] train_loss: 0.623\n",
      "[10,  1000] validation_loss: 1.312\n",
      "10 epoch, training accuracy: 0.6096\n",
      "10 epoch, validation accuracy: 0.5554\n",
      "Current best.\n",
      "-----------------------------------------\n",
      "[11,  1000] train_loss: 0.569\n",
      "[11,  1000] validation_loss: 1.256\n",
      "11 epoch, training accuracy: 0.6473\n",
      "11 epoch, validation accuracy: 0.5927\n",
      "Current best.\n",
      "-----------------------------------------\n",
      "[12,  1000] train_loss: 0.564\n",
      "[12,  1000] validation_loss: 1.252\n",
      "12 epoch, training accuracy: 0.6506\n",
      "12 epoch, validation accuracy: 0.5848\n",
      "-----------------------------------------\n",
      "[13,  1000] train_loss: 0.531\n",
      "[13,  1000] validation_loss: 1.312\n",
      "13 epoch, training accuracy: 0.6680\n",
      "13 epoch, validation accuracy: 0.5609\n",
      "-----------------------------------------\n",
      "[14,  1000] train_loss: 0.510\n",
      "[14,  1000] validation_loss: 1.495\n",
      "14 epoch, training accuracy: 0.6859\n",
      "14 epoch, validation accuracy: 0.5213\n",
      "-----------------------------------------\n",
      "[15,  1000] train_loss: 0.515\n",
      "[15,  1000] validation_loss: 1.327\n",
      "15 epoch, training accuracy: 0.6787\n",
      "15 epoch, validation accuracy: 0.5840\n",
      "-----------------------------------------\n",
      "[16,  1000] train_loss: 0.499\n",
      "[16,  1000] validation_loss: 1.224\n",
      "16 epoch, training accuracy: 0.6941\n",
      "16 epoch, validation accuracy: 0.6087\n",
      "Current best.\n",
      "-----------------------------------------\n",
      "[17,  1000] train_loss: 0.461\n",
      "[17,  1000] validation_loss: 1.146\n",
      "17 epoch, training accuracy: 0.7135\n",
      "17 epoch, validation accuracy: 0.6300\n",
      "Current best.\n",
      "-----------------------------------------\n",
      "[18,  1000] train_loss: 0.431\n",
      "[18,  1000] validation_loss: 1.238\n",
      "18 epoch, training accuracy: 0.7314\n",
      "18 epoch, validation accuracy: 0.5974\n",
      "-----------------------------------------\n",
      "[19,  1000] train_loss: 0.420\n",
      "[19,  1000] validation_loss: 1.227\n",
      "19 epoch, training accuracy: 0.7388\n",
      "19 epoch, validation accuracy: 0.6172\n",
      "-----------------------------------------\n",
      "[20,  1000] train_loss: 0.419\n",
      "[20,  1000] validation_loss: 1.226\n",
      "20 epoch, training accuracy: 0.7418\n",
      "20 epoch, validation accuracy: 0.6006\n",
      "-----------------------------------------\n",
      "[21,  1000] train_loss: 0.387\n",
      "[21,  1000] validation_loss: 1.307\n",
      "21 epoch, training accuracy: 0.7593\n",
      "21 epoch, validation accuracy: 0.6041\n",
      "-----------------------------------------\n",
      "[22,  1000] train_loss: 0.380\n",
      "[22,  1000] validation_loss: 1.303\n",
      "22 epoch, training accuracy: 0.7651\n",
      "22 epoch, validation accuracy: 0.5988\n",
      "-----------------------------------------\n",
      "[23,  1000] train_loss: 0.402\n",
      "[23,  1000] validation_loss: 1.182\n",
      "23 epoch, training accuracy: 0.7471\n",
      "23 epoch, validation accuracy: 0.6157\n",
      "-----------------------------------------\n",
      "[24,  1000] train_loss: 0.359\n",
      "[24,  1000] validation_loss: 1.208\n",
      "24 epoch, training accuracy: 0.7749\n",
      "24 epoch, validation accuracy: 0.6219\n",
      "-----------------------------------------\n",
      "[25,  1000] train_loss: 0.343\n",
      "[25,  1000] validation_loss: 1.147\n",
      "25 epoch, training accuracy: 0.7909\n",
      "25 epoch, validation accuracy: 0.6461\n",
      "Current best.\n",
      "-----------------------------------------\n",
      "[26,  1000] train_loss: 0.328\n",
      "[26,  1000] validation_loss: 1.365\n",
      "26 epoch, training accuracy: 0.7935\n",
      "26 epoch, validation accuracy: 0.5831\n",
      "-----------------------------------------\n",
      "[27,  1000] train_loss: 0.315\n",
      "[27,  1000] validation_loss: 1.164\n",
      "27 epoch, training accuracy: 0.8042\n",
      "27 epoch, validation accuracy: 0.6382\n",
      "-----------------------------------------\n",
      "[28,  1000] train_loss: 0.302\n",
      "[28,  1000] validation_loss: 1.291\n",
      "28 epoch, training accuracy: 0.8159\n",
      "28 epoch, validation accuracy: 0.6070\n",
      "-----------------------------------------\n",
      "[29,  1000] train_loss: 0.315\n",
      "[29,  1000] validation_loss: 1.244\n",
      "29 epoch, training accuracy: 0.8052\n",
      "29 epoch, validation accuracy: 0.6213\n",
      "-----------------------------------------\n",
      "[30,  1000] train_loss: 0.305\n",
      "[30,  1000] validation_loss: 1.202\n",
      "30 epoch, training accuracy: 0.8110\n",
      "30 epoch, validation accuracy: 0.6426\n",
      "-----------------------------------------\n",
      "[31,  1000] train_loss: 0.328\n",
      "[31,  1000] validation_loss: 1.256\n",
      "31 epoch, training accuracy: 0.7962\n",
      "31 epoch, validation accuracy: 0.6224\n",
      "-----------------------------------------\n",
      "[32,  1000] train_loss: 0.297\n",
      "[32,  1000] validation_loss: 1.120\n",
      "32 epoch, training accuracy: 0.8147\n",
      "32 epoch, validation accuracy: 0.6624\n",
      "Current best.\n",
      "-----------------------------------------\n",
      "[33,  1000] train_loss: 0.261\n",
      "[33,  1000] validation_loss: 1.159\n",
      "33 epoch, training accuracy: 0.8416\n",
      "33 epoch, validation accuracy: 0.6452\n",
      "-----------------------------------------\n",
      "[34,  1000] train_loss: 0.289\n",
      "[34,  1000] validation_loss: 1.143\n",
      "34 epoch, training accuracy: 0.8242\n",
      "34 epoch, validation accuracy: 0.6528\n",
      "-----------------------------------------\n",
      "[35,  1000] train_loss: 0.258\n",
      "[35,  1000] validation_loss: 1.142\n",
      "35 epoch, training accuracy: 0.8386\n",
      "35 epoch, validation accuracy: 0.6603\n",
      "-----------------------------------------\n",
      "[36,  1000] train_loss: 0.236\n",
      "[36,  1000] validation_loss: 1.232\n",
      "36 epoch, training accuracy: 0.8574\n",
      "36 epoch, validation accuracy: 0.6324\n",
      "-----------------------------------------\n",
      "[37,  1000] train_loss: 0.269\n",
      "[37,  1000] validation_loss: 1.291\n",
      "37 epoch, training accuracy: 0.8422\n",
      "37 epoch, validation accuracy: 0.6166\n",
      "-----------------------------------------\n",
      "[38,  1000] train_loss: 0.261\n",
      "[38,  1000] validation_loss: 1.125\n",
      "38 epoch, training accuracy: 0.8370\n",
      "38 epoch, validation accuracy: 0.6700\n",
      "Current best.\n",
      "-----------------------------------------\n",
      "[39,  1000] train_loss: 0.219\n",
      "[39,  1000] validation_loss: 1.232\n",
      "39 epoch, training accuracy: 0.8666\n",
      "39 epoch, validation accuracy: 0.6399\n",
      "-----------------------------------------\n",
      "[40,  1000] train_loss: 0.229\n",
      "[40,  1000] validation_loss: 1.133\n",
      "40 epoch, training accuracy: 0.8595\n",
      "40 epoch, validation accuracy: 0.6665\n",
      "-----------------------------------------\n",
      "[41,  1000] train_loss: 0.210\n",
      "[41,  1000] validation_loss: 1.181\n",
      "41 epoch, training accuracy: 0.8706\n",
      "41 epoch, validation accuracy: 0.6691\n",
      "-----------------------------------------\n",
      "[42,  1000] train_loss: 0.197\n",
      "[42,  1000] validation_loss: 1.199\n",
      "42 epoch, training accuracy: 0.8797\n",
      "42 epoch, validation accuracy: 0.6496\n",
      "-----------------------------------------\n",
      "[43,  1000] train_loss: 0.213\n",
      "[43,  1000] validation_loss: 1.287\n",
      "43 epoch, training accuracy: 0.8714\n",
      "43 epoch, validation accuracy: 0.6411\n",
      "-----------------------------------------\n",
      "[44,  1000] train_loss: 0.198\n",
      "[44,  1000] validation_loss: 1.233\n",
      "44 epoch, training accuracy: 0.8797\n",
      "44 epoch, validation accuracy: 0.6475\n",
      "-----------------------------------------\n",
      "[45,  1000] train_loss: 0.187\n",
      "[45,  1000] validation_loss: 1.373\n",
      "45 epoch, training accuracy: 0.8915\n",
      "45 epoch, validation accuracy: 0.6353\n",
      "-----------------------------------------\n",
      "[46,  1000] train_loss: 0.185\n",
      "[46,  1000] validation_loss: 1.215\n",
      "46 epoch, training accuracy: 0.8860\n",
      "46 epoch, validation accuracy: 0.6673\n",
      "Early Stop.\n",
      "Best epoch is 38\n",
      "Finished Training\n",
      "-----------------------------------------\n",
      "Reload ./model//38.pth model.\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/weienv/lib/python3.7/site-packages/PIL/Image.py:2766: DecompressionBombWarning: Image size (91049764 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  DecompressionBombWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Top 1 Accuracy: 2276/3347 (68.00%), Top 3 Accuracy: 3012/3347 (89.99%)\n",
      "Bread                : 225/368     61.141304%\n",
      "Dairy_product        : 69/148     46.621622%\n",
      "Dessert              : 239/500     47.800000%\n",
      "Egg                  : 176/335     52.537313%\n",
      "Fried_food           : 217/287     75.609756%\n",
      "Meat                 : 335/432     77.546296%\n",
      "Noodles              : 99/147     67.346939%\n",
      "Rice                 : 50/96     52.083333%\n",
      "Seafood              : 250/303     82.508251%\n",
      "Soup                 : 433/500     86.600000%\n",
      "Vegetable_fruit      : 183/231     79.220779%\n",
      "Average per case accuracy:  66.274145%\n",
      "-----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    config = Config()\n",
    "    #The transform function for train data\n",
    "    transform_train = trans.Compose([\n",
    "        ImgAugTransform(config),\n",
    "        trans.ToTensor(),\n",
    "        trans.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    #The transform function for validation data\n",
    "    transform_validation = trans.Compose([\n",
    "        trans.Resize((config.image_size, config.image_size)),\n",
    "        trans.ToTensor(),\n",
    "        trans.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    #The transform function for test data\n",
    "    transform_test = trans.Compose([\n",
    "        trans.Resize((config.image_size, config.image_size)),\n",
    "        trans.ToTensor(),\n",
    "        trans.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "\n",
    "    train_dataset = torchvision.datasets.ImageFolder(root = config.train_dataset_path ,transform=transform_train)\n",
    "    validation_dataset = torchvision.datasets.ImageFolder(root = config.validation_dataset_path ,transform=transform_validation)\n",
    "    evaluation_dataset = torchvision.datasets.ImageFolder(root = config.test_dataset_path ,transform=transform_test)\n",
    "\n",
    "    sampler = WRSampler(train_dataset, config.wts)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=config.train_batchsize, sampler=sampler)\n",
    "    validation_loader = torch.utils.data.DataLoader(validation_dataset, batch_size=config.eva_val_batchsize, shuffle=False)\n",
    "    evaluation_loader = torch.utils.data.DataLoader(evaluation_dataset, batch_size=config.eva_val_batchsize, shuffle=False)\n",
    "\n",
    "    if config.net == 'resnet18':\n",
    "        net = models.resnet18(pretrained=config.pretrain)\n",
    "        net.fc = nn.Sequential(nn.Linear(512,256),nn.LeakyReLU(),nn.Linear(256,128),nn.LeakyReLU(),nn.Linear(128,config.class_num))\n",
    "        net = net.to(device) \n",
    "    elif config.net == 'densenet121':\n",
    "        net = models.densenet121(pretrained=config.pretrain)\n",
    "        net.fc = nn.Sequential(nn.Linear(1000,256),nn.LeakyReLU(),nn.Linear(256,128),nn.LeakyReLU(),nn.Linear(128,config.class_num))\n",
    "        net = net.to(device) \n",
    "    elif config.net == 'resnet101':\n",
    "        net = models.resnet101(pretrained=config.pretrain)\n",
    "        net.fc = nn.Sequential(nn.Linear(2048,256),nn.LeakyReLU(),nn.Linear(256,128),nn.LeakyReLU(),nn.Linear(128,config.class_num))\n",
    "        net = net.to(device) \n",
    "\n",
    "    config.folder_names2code = train_dataset.class_to_idx\n",
    "    max_epoch = config.max_epoch\n",
    "    learning_rate = config.lr\n",
    "    criterion = config.criterion\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate, betas=[0.9, 0.999]) #定優化函數\n",
    "    \n",
    "    #train(net, criterion, optimizer, max_epoch, train_loader, validation_loader, config)\n",
    "    teacher_model = reload_teacher_net(config).to(device)\n",
    "    trainKD(net, teacher_model, criterion, optimizer, max_epoch, train_loader, validation_loader, config)\n",
    "    \n",
    "    pretrain_net = reload_net(config).to(device)\n",
    "    evaluation(pretrain_net, evaluation_dataset, evaluation_loader, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "Reload ./model//38.pth model.\n",
      "-----------------------------------------\n",
      "Test set: Top 1 Accuracy: 2276/3347 (68.00%), Top 3 Accuracy: 3012/3347 (89.99%)\n",
      "Bread                : 225/368     61.141304%\n",
      "Dairy_product        : 69/148     46.621622%\n",
      "Dessert              : 239/500     47.800000%\n",
      "Egg                  : 176/335     52.537313%\n",
      "Fried_food           : 217/287     75.609756%\n",
      "Meat                 : 335/432     77.546296%\n",
      "Noodles              : 99/147     67.346939%\n",
      "Rice                 : 50/96     52.083333%\n",
      "Seafood              : 250/303     82.508251%\n",
      "Soup                 : 433/500     86.600000%\n",
      "Vegetable_fruit      : 183/231     79.220779%\n",
      "Average per case accuracy:  66.274145%\n",
      "-----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "pretrain_net = reload_net(config).to(device)\n",
    "evaluation(pretrain_net, evaluation_dataset, evaluation_loader, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "x1 = [0, 1, 2, 3, 4]\n",
    "y1 = [11, 30, 14, 13, 46]\n",
    "\n",
    "plt.plot(x1, y1, '-', label='Epoch', markersize=10)\n",
    "plt.xlabel('Learning Rate')\n",
    "plt.ylabel('Epoch')\n",
    "plt.legend()\n",
    "plt.xticks([0, 1, 2, 3, 4], ['0.1', '0.01', '0.001', '0.0001', '0.00001'])\n",
    "#plt.savefig('mAP_lrc.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (6): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (7): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (8): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (9): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (10): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (11): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (12): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (13): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (14): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (15): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (16): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (17): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (18): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (19): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (20): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (21): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (22): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models.resnet101(pretrained=config.pretrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
