{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import join, splitext, basename\n",
    "import glob\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as trans\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import Sampler\n",
    "from PIL import Image\n",
    "from imgaug import augmenters as iaa\n",
    "from matplotlib.pyplot import figure, imshow, axis\n",
    "import imgaug as ia\n",
    "import numpy as np\n",
    "import PIL\n",
    "import torch\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import statistics\n",
    "import random\n",
    "import natsort\n",
    "import copy\n",
    "import collections\n",
    "import torchvision.models as models\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Check devices..\n",
      "Current device:  cuda\n",
      "Our selected device:  0\n",
      "1  GPUs is available\n"
     ]
    }
   ],
   "source": [
    "#To determine if your system supports CUDA\n",
    "print(\"==> Check devices..\")\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"Current device: \",device)\n",
    "\n",
    "#Also can print your current GPU id, and the number of GPUs you can use.\n",
    "print(\"Our selected device: \", torch.cuda.current_device())\n",
    "print(torch.cuda.device_count(), \" GPUs is available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config():\n",
    "    def __init__(self):\n",
    "        self.FolderNames2English_names = {\n",
    "                                    '0':\"Bread\",          # 994 \n",
    "                                    '1':\"Dairy_product\",  # 429\n",
    "                                    '2':\"Dessert\",        # 1500\n",
    "                                    '3':\"Egg\",            # 986\n",
    "                                    '4':\"Fried_food\",     # 848\n",
    "                                    '5':\"Meat\",           # 1325\n",
    "                                    '6':\"Noodles\",        # 440\n",
    "                                    '7':\"Rice\",           # 280\n",
    "                                    '8':\"Seafood\",        # 855\n",
    "                                    '9':\"Soup\",           # 1500\n",
    "                                    '10':\"Vegetable_fruit\"# 709\n",
    "                                    }\n",
    "        self.folder_names2code = {}\n",
    "        self.image_size = 224\n",
    "        self.early_stop = 5\n",
    "        self.max_epoch = 1000\n",
    "        self.train_batchsize = 128\n",
    "        self.eva_val_batchsize = 32\n",
    "        self.class_num = 11\n",
    "        self.each_class_item_num = {}\n",
    "        self.temperature = 1\n",
    "        self.alpha = 0.5\n",
    "        \n",
    "        \n",
    "        self.train_dataset_path = r'./food11re/training'\n",
    "        self.validation_dataset_path = r'./food11re/validation'\n",
    "        self.test_dataset_path = r'./food11re/evaluation'\n",
    "        self.model_ouput_dir = './model/'\n",
    "        self.teacher_model_path = './teacher_model/6.pth'\n",
    "        self.best_epoch = 0\n",
    "        class_folder_name = listdir(self.test_dataset_path)\n",
    "        self.class_folder_num = {}\n",
    "        for cf in class_folder_name:\n",
    "            self.class_folder_num[cf] = len(listdir(self.test_dataset_path + '/' + cf))\n",
    "            \n",
    "        \n",
    "        self.net = 'resnet18'  # 0: resnet18\n",
    "        self.pretrain = True\n",
    "\n",
    "        self.wts = [500/self.class_folder_num['0'], 500/self.class_folder_num['1'], 500/self.class_folder_num['2'], \n",
    "                    500/self.class_folder_num['3'], 500/self.class_folder_num['4'], 500/self.class_folder_num['5'], \n",
    "                    500/self.class_folder_num['6'], 500/self.class_folder_num['7'], 500/self.class_folder_num['8'], \n",
    "                    500/self.class_folder_num['9'], 500/self.class_folder_num['10']]\n",
    "        self.lr = 0.0001\n",
    "        self.criterion = nn.CrossEntropyLoss() #定義損失函數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_image_file(filename):\n",
    "    return any(filename.endswith(extension) for extension in [\".png\", \".jpg\", \".jpeg\"])\n",
    "\n",
    "def load_img(filepath):\n",
    "    img = Image.open(filepath)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImgAugTransform():\n",
    "    def __init__(self, config=Config()):\n",
    "        self.aug = iaa.Sequential([\n",
    "            iaa.Scale((config.image_size, config.image_size)),\n",
    "            iaa.Sometimes(0.25, iaa.GaussianBlur(sigma=(0, 3.0))),\n",
    "            iaa.Fliplr(0.5),\n",
    "            iaa.Affine(rotate=(-20, 20), mode='symmetric'),\n",
    "            iaa.Sometimes(0.25,\n",
    "                      iaa.OneOf([iaa.Dropout(p=(0, 0.1)),\n",
    "                                 iaa.CoarseDropout(0.1, size_percent=0.5)])),  # 對batch中的一部分圖片應用一部分Augmenters,剩下的圖片應用另外的Augmenters。\n",
    "            iaa.AddToHueAndSaturation(value=(-10, 10), per_channel=True)  # 即修改色調和飽和度\n",
    "        ])\n",
    "      \n",
    "    def __call__(self, img):\n",
    "        img = np.array(img)\n",
    "        return self.aug.augment_image(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WRSampler(dataset, wts):\n",
    "    class_name_list = dataset.classes\n",
    "    num_per_classes = {}\n",
    "    for img in dataset.imgs:\n",
    "        if  img[1] not in num_per_classes:\n",
    "            num_per_classes[int(img[1])] = 1\n",
    "        else:\n",
    "            num_per_classes[int(img[1])] += 1\n",
    "            \n",
    "    each_data_wts = []\n",
    "    for class_name in class_name_list:\n",
    "        class_item_num = num_per_classes[int(class_name)]\n",
    "        for i in range(class_item_num):\n",
    "            each_data_wts.append(wts[int(class_name)])\n",
    "    \n",
    "    sampler = torch.utils.data.sampler.WeightedRandomSampler(each_data_wts, len(each_data_wts), replacement=True)\n",
    "    \n",
    "    return sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, criterion, optimizer, max_epoch, train_loader, validation_loader, config):\n",
    "    t_loss = []\n",
    "    v_loss = []\n",
    "    training_accuracy = []\n",
    "    validation_accuracy = []\n",
    "    total = 0\n",
    "    min_val_loss = 0.0\n",
    "    min_val_error = 0.0\n",
    "    early_stop_timer = 0 \n",
    "\n",
    "    for epoch in range(max_epoch):  # loop over the dataset multiple times\n",
    "        train_loss = 0.0\n",
    "        validation_loss = 0.0\n",
    "        correct_train = 0\n",
    "        correct_validation = 0\n",
    "        train_num = 0\n",
    "        val_num = 0\n",
    "        train_img_num = 0\n",
    "        validation_img_num = 0\n",
    "\n",
    "\n",
    "        ########################\n",
    "        # train the model      #\n",
    "        ########################\n",
    "\n",
    "        model.train()\n",
    "        for i, (inputs, labels) in enumerate(train_loader, 0):\n",
    "\n",
    "            #change the type into cuda tensor \n",
    "            inputs = inputs.to(device) \n",
    "            labels = labels.to(device) \n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(inputs)\n",
    "            # select the class with highest probability\n",
    "            _, pred = outputs.max(1)\n",
    "            # if the model predicts the same results as the true\n",
    "            # label, then the correct counter will plus 1\n",
    "            correct_train += pred.eq(labels).sum().item()\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            train_loss += loss.item()\n",
    "            train_num += 1\n",
    "            train_img_num += len(labels)\n",
    "\n",
    "\n",
    "        ########################\n",
    "        # validate the model   #\n",
    "        ########################\n",
    "\n",
    "        model.eval()\n",
    "        for i, (inputs, labels) in enumerate(validation_loader, 0):\n",
    "            # move tensors to GPU if CUDA is available\n",
    "            inputs = inputs.to(device) \n",
    "            labels = labels.to(device)\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            outputs = model(inputs)\n",
    "            _, pred = outputs.max(1)\n",
    "            correct_validation += pred.eq(labels).sum().item()\n",
    "            # calculate the batch loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            # update average validation loss \n",
    "            validation_loss += loss.item()\n",
    "            val_num += 1\n",
    "            validation_img_num += len(labels)\n",
    "\n",
    "\n",
    "        if epoch % 1 == 0:    # print every 200 mini-batches\n",
    "            val_error = 1 - correct_validation / validation_img_num\n",
    "            print('[%d, %5d] train_loss: %.3f' % (epoch, max_epoch, train_loss / train_num))\n",
    "            print('[%d, %5d] validation_loss: %.3f' % (epoch, max_epoch, validation_loss / val_num))\n",
    "            print('%d epoch, training accuracy: %.4f' % (epoch, correct_train / train_img_num))\n",
    "            print('%d epoch, validation accuracy: %.4f' % (epoch, correct_validation / validation_img_num))\n",
    "\n",
    "\n",
    "            if epoch == 0:\n",
    "                min_val_error = val_error\n",
    "                print('Current best.')\n",
    "\n",
    "            if val_error < min_val_error:\n",
    "                min_val_error = val_error\n",
    "                config.best_epoch = epoch\n",
    "                early_stop_timer = 0\n",
    "                print('Current best.')\n",
    "            else:\n",
    "                early_stop_timer += 1\n",
    "                if early_stop_timer >= config.early_stop:\n",
    "                    print('Early Stop.\\n Best epoch is', str(config.best_epoch))\n",
    "                    break\n",
    "            t_loss.append(train_loss / train_num)\n",
    "            training_accuracy.append(correct_train / train_img_num)\n",
    "            validation_accuracy.append(correct_validation / validation_img_num)\n",
    "            running_loss = 0.0\n",
    "            validation_loss = 0.0\n",
    "            train_num = 0\n",
    "            val_num = 0\n",
    "            correct_train = 0\n",
    "            correct_validation = 0\n",
    "            total = 0\n",
    "            print('-----------------------------------------')\n",
    "\n",
    "            torch.save(model.state_dict(), config.model_ouput_dir + str(epoch) + '.pth')\n",
    "\n",
    "    print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_teacher_outputs(teacher_model, inputs):\n",
    "    teacher_model.eval()   \n",
    "    teacher_outputs = teacher_model(inputs)\n",
    "    \n",
    "    return teacher_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn_kd(outputs, labels, teacher_outputs, config):\n",
    "    \"\"\"\n",
    "    outputs        : training prediction of inputs.\n",
    "    labels         : hard labels.\n",
    "    teacher_outputs: soft labels.\n",
    "    config         : config including alpha and temperature.\n",
    "    \"\"\"\n",
    "    \n",
    "    alpha = config.alpha\n",
    "    temperature = config.temperature\n",
    "    \n",
    "    KD_loss = nn.KLDivLoss()(nn.functional.log_softmax(outputs/temperature, dim=1),\n",
    "                             nn.functional.softmax(teacher_outputs/temperature, dim=1)) * (alpha * temperature * temperature) + \\\n",
    "                             nn.functional.cross_entropy(outputs, labels) * (1. - alpha)\n",
    "\n",
    "    return KD_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainKD(model, teacher_model, criterion, optimizer, max_epoch, train_loader, validation_loader, config):\n",
    "    t_loss = []\n",
    "    v_loss = []\n",
    "    training_accuracy = []\n",
    "    validation_accuracy = []\n",
    "    total = 0\n",
    "    min_val_loss = 0.0\n",
    "    min_val_error = 0.0\n",
    "    early_stop_timer = 0 \n",
    "\n",
    "\n",
    "    for epoch in range(max_epoch):  # loop over the dataset multiple times\n",
    "        train_loss = 0.0\n",
    "        validation_loss = 0.0\n",
    "        correct_train = 0\n",
    "        correct_validation = 0\n",
    "        train_num = 0\n",
    "        val_num = 0\n",
    "        train_img_num = 0\n",
    "        validation_img_num = 0\n",
    "\n",
    "\n",
    "        ########################\n",
    "        # train the model      #\n",
    "        ########################\n",
    "\n",
    "        model.train()\n",
    "        for i, (inputs, labels) in enumerate(train_loader, 0):\n",
    "\n",
    "            #change the type into cuda tensor \n",
    "            inputs = inputs.to(device) \n",
    "            labels = labels.to(device) \n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(inputs)\n",
    "            teacher_outputs = get_teacher_outputs(teacher_model, inputs)\n",
    "            \n",
    "            _, pred = outputs.max(1)\n",
    "            # if the model predicts the same results as the true\n",
    "            # label, then the correct counter will plus 1\n",
    "            correct_train += pred.eq(labels).sum().item()\n",
    "            loss = loss_fn_kd(outputs, labels, teacher_outputs, config)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            train_loss += loss.item()\n",
    "            train_num += 1\n",
    "            train_img_num += len(labels)\n",
    "\n",
    "\n",
    "        ########################\n",
    "        # validate the model   #\n",
    "        ########################\n",
    "\n",
    "        model.eval()\n",
    "        for i, (inputs, labels) in enumerate(validation_loader, 0):\n",
    "            # move tensors to GPU if CUDA is available\n",
    "            inputs = inputs.to(device) \n",
    "            labels = labels.to(device)\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            outputs = model(inputs)\n",
    "            _, pred = outputs.max(1)\n",
    "            correct_validation += pred.eq(labels).sum().item()\n",
    "            # calculate the batch loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            # update average validation loss \n",
    "            validation_loss += loss.item()\n",
    "            val_num += 1\n",
    "            validation_img_num += len(labels)\n",
    "\n",
    "\n",
    "        if epoch % 1 == 0:    # print every 200 mini-batches\n",
    "            val_error = 1 - correct_validation / validation_img_num\n",
    "            print('[%d, %5d] train_loss: %.3f' % (epoch, max_epoch, train_loss / train_num))\n",
    "            print('[%d, %5d] validation_loss: %.3f' % (epoch, max_epoch, validation_loss / val_num))\n",
    "            print('%d epoch, training accuracy: %.4f' % (epoch, correct_train / train_img_num))\n",
    "            print('%d epoch, validation accuracy: %.4f' % (epoch, correct_validation / validation_img_num))\n",
    "\n",
    "\n",
    "            if epoch == 0:\n",
    "                min_val_error = val_error\n",
    "                print('Current best.')\n",
    "                \n",
    "            if val_error < min_val_error:\n",
    "                min_val_error = val_error\n",
    "                config.best_epoch = epoch\n",
    "                early_stop_timer = 0\n",
    "                print('Current best.')\n",
    "            else:\n",
    "                early_stop_timer += 1\n",
    "                if early_stop_timer >= config.early_stop:\n",
    "                    print('Early Stop.\\nBest epoch is', str(config.best_epoch))\n",
    "                    break\n",
    "            t_loss.append(train_loss / train_num)\n",
    "            training_accuracy.append(correct_train / train_img_num)\n",
    "            validation_accuracy.append(correct_validation / validation_img_num)\n",
    "            running_loss = 0.0\n",
    "            validation_loss = 0.0\n",
    "            train_num = 0\n",
    "            val_num = 0\n",
    "            correct_train = 0\n",
    "            correct_validation = 0\n",
    "            total = 0\n",
    "            print('-----------------------------------------')\n",
    "\n",
    "            torch.save(model.state_dict(), config.model_ouput_dir + str(epoch) + '.pth')\n",
    "\n",
    "    print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reload_net(config):\n",
    "    if config.net == 'resnet18':\n",
    "        net = models.resnet18(pretrained=False)\n",
    "        net.fc = nn.Sequential(nn.Linear(512,256),nn.LeakyReLU(),nn.Linear(256,128),nn.LeakyReLU(),nn.Linear(128,config.class_num))\n",
    "        print('-----------------------------------------')\n",
    "        print('Reload', config.model_ouput_dir + '/' + str(config.best_epoch) + '.pth model.')\n",
    "        print('-----------------------------------------')\n",
    "        net.load_state_dict(torch.load(config.model_ouput_dir + '/' + str(config.best_epoch) + '.pth'))\n",
    "    \n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reload_teacher_net(config):\n",
    "    if config.net == 'resnet18':\n",
    "        net = models.resnet18(pretrained=False)\n",
    "        net.fc = nn.Sequential(nn.Linear(512,256),nn.LeakyReLU(),nn.Linear(256,128),nn.LeakyReLU(),nn.Linear(128,config.class_num))\n",
    "        print('-----------------------------------------')\n",
    "        print('Reload', config.teacher_model_path, 'model.')\n",
    "        print('-----------------------------------------')\n",
    "        net.load_state_dict(torch.load(config.teacher_model_path))\n",
    "    \n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(model, evaluation_loader, config):\n",
    "    test_loss = 0.0\n",
    "    correct_test = 0\n",
    "    test_num = 0\n",
    "    cls = np.zeros(config.class_num)\n",
    "    correct_top3 = 0\n",
    "    class_folder_num = config.class_folder_num\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    for i, (inputs, labels) in enumerate(evaluation_loader, 0):\n",
    "        # move tensors to GPU if CUDA is available\n",
    "        inputs = inputs.to(device) \n",
    "        labels = labels.to(device)\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        outputs = model(inputs)\n",
    "        _, pred = outputs.max(1)\n",
    "        correct_test += pred.eq(labels).sum().item()\n",
    "        _, top3 = outputs.topk(config.class_num)\n",
    "        correct_top3 += top3.eq(labels.view(-1,1).expand_as(top3)).sum().item()\n",
    "\n",
    "        for j in range(config.class_num):\n",
    "            cls[j] += (pred.eq(j) * pred.eq(labels)).sum().item()\n",
    "\n",
    "    print('Test set: Top 1 Accuracy: %d/%d (%.2f%%), Top 3 Accuracy: %d/%d (%.2f%%)' \n",
    "          % (correct_test, len(evaluation_dataset), correct_test / len(evaluation_dataset)*100, correct_top3, len(evaluation_dataset),\n",
    "             correct_top3/ len(evaluation_dataset)*100))\n",
    "\n",
    "    FN2EN = config.FolderNames2English_names\n",
    "\n",
    "    print('%-20s : %d/%d    %10f%%' % (FN2EN['0'], cls[config.folder_names2code['0']], class_folder_num['0'], cls[config.folder_names2code['0']]/class_folder_num['0']*100))                                                    \n",
    "    print('%-20s : %d/%d    %10f%%' % (FN2EN['1'], cls[config.folder_names2code['1']], class_folder_num['1'], cls[config.folder_names2code['1']]/class_folder_num['1']*100))\n",
    "    print('%-20s : %d/%d    %10f%%' % (FN2EN['2'], cls[config.folder_names2code['2']], class_folder_num['2'], cls[config.folder_names2code['2']]/class_folder_num['2']*100))\n",
    "    print('%-20s : %d/%d    %10f%%' % (FN2EN['3'], cls[config.folder_names2code['3']], class_folder_num['3'], cls[config.folder_names2code['3']]/class_folder_num['3']*100))\n",
    "    print('%-20s : %d/%d    %10f%%' % (FN2EN['4'], cls[config.folder_names2code['4']], class_folder_num['4'], cls[config.folder_names2code['4']]/class_folder_num['4']*100))\n",
    "    print('%-20s : %d/%d    %10f%%' % (FN2EN['5'], cls[config.folder_names2code['5']], class_folder_num['5'], cls[config.folder_names2code['5']]/class_folder_num['5']*100))\n",
    "    print('%-20s : %d/%d    %10f%%' % (FN2EN['6'], cls[config.folder_names2code['6']], class_folder_num['6'], cls[config.folder_names2code['6']]/class_folder_num['6']*100))\n",
    "    print('%-20s : %d/%d    %10f%%' % (FN2EN['7'], cls[config.folder_names2code['7']], class_folder_num['7'], cls[config.folder_names2code['7']]/class_folder_num['7']*100))\n",
    "    print('%-20s : %d/%d    %10f%%' % (FN2EN['8'], cls[config.folder_names2code['8']], class_folder_num['8'], cls[config.folder_names2code['8']]/class_folder_num['8']*100))\n",
    "    print('%-20s : %d/%d    %10f%%' % (FN2EN['9'], cls[config.folder_names2code['9']], class_folder_num['9'], cls[config.folder_names2code['9']]/class_folder_num['9']*100))\n",
    "    print('%-20s : %d/%d    %10f%%' % (FN2EN['10'], cls[config.folder_names2code['10']], class_folder_num['10'], cls[config.folder_names2code['10']]/class_folder_num['10']*100))\n",
    "\n",
    "\n",
    "    avg = []\n",
    "    avg.append(cls[config.folder_names2code['0']]/class_folder_num['0']*100)\n",
    "    avg.append(cls[config.folder_names2code['1']]/class_folder_num['1']*100)\n",
    "    avg.append(cls[config.folder_names2code['2']]/class_folder_num['2']*100)\n",
    "    avg.append(cls[config.folder_names2code['3']]/class_folder_num['3']*100)\n",
    "    avg.append(cls[config.folder_names2code['4']]/class_folder_num['4']*100)\n",
    "    avg.append(cls[config.folder_names2code['5']]/class_folder_num['5']*100)\n",
    "    avg.append(cls[config.folder_names2code['6']]/class_folder_num['6']*100)\n",
    "    avg.append(cls[config.folder_names2code['7']]/class_folder_num['7']*100)\n",
    "    avg.append(cls[config.folder_names2code['8']]/class_folder_num['8']*100)\n",
    "    avg.append(cls[config.folder_names2code['9']]/class_folder_num['9']*100)\n",
    "    avg.append(cls[config.folder_names2code['10']]/class_folder_num['10']*100)\n",
    "\n",
    "    print('Average per case accuracy: %10f%%' % (sum(avg)/len(avg)))\n",
    "    print('-----------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/weienv/lib/python3.7/site-packages/imgaug/imgaug.py:184: DeprecationWarning: Function `Scale()` is deprecated. Use `Resize` instead. Resize has the exactly same interface as Scale.\n",
      "  warn_deprecated(msg, stacklevel=3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0,  1000] train_loss: 1.511\n",
      "[0,  1000] validation_loss: 0.777\n",
      "0 epoch, training accuracy: 0.5602\n",
      "0 epoch, validation accuracy: 0.7755\n",
      "Current best.\n",
      "-----------------------------------------\n",
      "[1,  1000] train_loss: 0.580\n",
      "[1,  1000] validation_loss: 0.503\n",
      "1 epoch, training accuracy: 0.8199\n",
      "1 epoch, validation accuracy: 0.8449\n",
      "Current best.\n",
      "-----------------------------------------\n",
      "[2,  1000] train_loss: 0.427\n",
      "[2,  1000] validation_loss: 0.445\n",
      "2 epoch, training accuracy: 0.8608\n",
      "2 epoch, validation accuracy: 0.8624\n",
      "Current best.\n",
      "-----------------------------------------\n",
      "[3,  1000] train_loss: 0.334\n",
      "[3,  1000] validation_loss: 0.430\n",
      "3 epoch, training accuracy: 0.8990\n",
      "3 epoch, validation accuracy: 0.8630\n",
      "Current best.\n",
      "-----------------------------------------\n",
      "[4,  1000] train_loss: 0.302\n",
      "[4,  1000] validation_loss: 0.409\n",
      "4 epoch, training accuracy: 0.9064\n",
      "4 epoch, validation accuracy: 0.8665\n",
      "Current best.\n",
      "-----------------------------------------\n",
      "[5,  1000] train_loss: 0.243\n",
      "[5,  1000] validation_loss: 0.433\n",
      "5 epoch, training accuracy: 0.9205\n",
      "5 epoch, validation accuracy: 0.8638\n",
      "-----------------------------------------\n",
      "[6,  1000] train_loss: 0.203\n",
      "[6,  1000] validation_loss: 0.399\n",
      "6 epoch, training accuracy: 0.9348\n",
      "6 epoch, validation accuracy: 0.8741\n",
      "Current best.\n",
      "-----------------------------------------\n",
      "[7,  1000] train_loss: 0.158\n",
      "[7,  1000] validation_loss: 0.413\n",
      "7 epoch, training accuracy: 0.9504\n",
      "7 epoch, validation accuracy: 0.8816\n",
      "Current best.\n",
      "-----------------------------------------\n",
      "[8,  1000] train_loss: 0.150\n",
      "[8,  1000] validation_loss: 0.442\n",
      "8 epoch, training accuracy: 0.9507\n",
      "8 epoch, validation accuracy: 0.8697\n",
      "-----------------------------------------\n",
      "[9,  1000] train_loss: 0.146\n",
      "[9,  1000] validation_loss: 0.507\n",
      "9 epoch, training accuracy: 0.9544\n",
      "9 epoch, validation accuracy: 0.8534\n",
      "-----------------------------------------\n",
      "[10,  1000] train_loss: 0.161\n",
      "[10,  1000] validation_loss: 0.410\n",
      "10 epoch, training accuracy: 0.9481\n",
      "10 epoch, validation accuracy: 0.8764\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-fd2cfce8c5c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbetas\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.999\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#定優化函數\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m     \u001b[0;31m#teacher_model = reload_teacher_net(config).to(device)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;31m#trainKD(net, teacher_model, criterion, optimizer, max_epoch, train_loader, validation_loader, config)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-b51b2218495f>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, criterion, optimizer, max_epoch, train_loader, validation_loader, config)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0;31m#change the type into cuda tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/weienv/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/weienv/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/weienv/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/weienv/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/weienv/lib/python3.7/site-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m             \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/weienv/lib/python3.7/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-5ca6bb2d7aff>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maug\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maugment_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/weienv/lib/python3.7/site-packages/imgaug/augmenters/meta.py\u001b[0m in \u001b[0;36maugment_image\u001b[0;34m(self, image, hooks)\u001b[0m\n\u001b[1;32m    769\u001b[0m             \"got shape %s.\" % (image.shape,))\n\u001b[1;32m    770\u001b[0m         \u001b[0miabase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_warn_on_suspicious_single_image_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 771\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maugment_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhooks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    772\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    773\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0maugment_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/weienv/lib/python3.7/site-packages/imgaug/augmenters/meta.py\u001b[0m in \u001b[0;36maugment_images\u001b[0;34m(self, images, parents, hooks)\u001b[0m\n\u001b[1;32m    823\u001b[0m             \u001b[0mUnnormalizedBatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m             \u001b[0mparents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparents\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m             \u001b[0mhooks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    826\u001b[0m         ).images_aug\n\u001b[1;32m    827\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/weienv/lib/python3.7/site-packages/imgaug/augmenters/meta.py\u001b[0m in \u001b[0;36maugment_batch_\u001b[0;34m(self, batch, parents, hooks)\u001b[0m\n\u001b[1;32m    643\u001b[0m                     \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m                     \u001b[0mparents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparents\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mparents\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 645\u001b[0;31m                     hooks=hooks)\n\u001b[0m\u001b[1;32m    646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m         \u001b[0;31m# revert augmentables being set to None for non-activated augmenters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/weienv/lib/python3.7/site-packages/imgaug/augmenters/meta.py\u001b[0m in \u001b[0;36m_augment_batch_\u001b[0;34m(self, batch, random_state, parents, hooks)\u001b[0m\n\u001b[1;32m   3140\u001b[0m                     \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3141\u001b[0m                     \u001b[0mparents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparents\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3142\u001b[0;31m                     \u001b[0mhooks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3143\u001b[0m                 )\n\u001b[1;32m   3144\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/weienv/lib/python3.7/site-packages/imgaug/augmenters/meta.py\u001b[0m in \u001b[0;36maugment_batch_\u001b[0;34m(self, batch, parents, hooks)\u001b[0m\n\u001b[1;32m    643\u001b[0m                     \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m                     \u001b[0mparents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparents\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mparents\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 645\u001b[0;31m                     hooks=hooks)\n\u001b[0m\u001b[1;32m    646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m         \u001b[0;31m# revert augmentables being set to None for non-activated augmenters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/weienv/lib/python3.7/site-packages/imgaug/augmenters/meta.py\u001b[0m in \u001b[0;36m_augment_batch_\u001b[0;34m(self, batch, random_state, parents, hooks)\u001b[0m\n\u001b[1;32m   3649\u001b[0m                         \u001b[0mbatch_sub\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3650\u001b[0m                         \u001b[0mparents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparents\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3651\u001b[0;31m                         \u001b[0mhooks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3652\u001b[0m                     )\n\u001b[1;32m   3653\u001b[0m                     batch = batch.invert_subselect_rows_by_indices_(indices,\n",
      "\u001b[0;32m~/anaconda3/envs/weienv/lib/python3.7/site-packages/imgaug/augmenters/meta.py\u001b[0m in \u001b[0;36maugment_batch_\u001b[0;34m(self, batch, parents, hooks)\u001b[0m\n\u001b[1;32m    643\u001b[0m                     \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m                     \u001b[0mparents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparents\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mparents\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 645\u001b[0;31m                     hooks=hooks)\n\u001b[0m\u001b[1;32m    646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m         \u001b[0;31m# revert augmentables being set to None for non-activated augmenters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/weienv/lib/python3.7/site-packages/imgaug/augmenters/meta.py\u001b[0m in \u001b[0;36m_augment_batch_\u001b[0;34m(self, batch, random_state, parents, hooks)\u001b[0m\n\u001b[1;32m   3140\u001b[0m                     \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3141\u001b[0m                     \u001b[0mparents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparents\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3142\u001b[0;31m                     \u001b[0mhooks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3143\u001b[0m                 )\n\u001b[1;32m   3144\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/weienv/lib/python3.7/site-packages/imgaug/augmenters/meta.py\u001b[0m in \u001b[0;36maugment_batch_\u001b[0;34m(self, batch, parents, hooks)\u001b[0m\n\u001b[1;32m    643\u001b[0m                     \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m                     \u001b[0mparents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparents\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mparents\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 645\u001b[0;31m                     hooks=hooks)\n\u001b[0m\u001b[1;32m    646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m         \u001b[0;31m# revert augmentables being set to None for non-activated augmenters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/weienv/lib/python3.7/site-packages/imgaug/augmenters/blur.py\u001b[0m in \u001b[0;36m_augment_batch_\u001b[0;34m(self, batch, random_state, parents, hooks)\u001b[0m\n\u001b[1;32m    466\u001b[0m                                           random_state=random_state)\n\u001b[1;32m    467\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msig\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 468\u001b[0;31m             \u001b[0mimage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblur_gaussian_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    469\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/weienv/lib/python3.7/site-packages/imgaug/augmenters/blur.py\u001b[0m in \u001b[0;36mblur_gaussian_\u001b[0;34m(image, sigma, ksize, backend, eps)\u001b[0m\n\u001b[1;32m    252\u001b[0m                     \u001b[0msigmaX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msigma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                     \u001b[0msigmaY\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msigma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m                     borderType=cv2.BORDER_REFLECT_101)\n\u001b[0m\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m                 \u001b[0;31m# re-add channel axis removed by cv2 if input was (H, W, 1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    config = Config()\n",
    "    #The transform function for train data\n",
    "    transform_train = trans.Compose([\n",
    "        ImgAugTransform(config),\n",
    "        trans.ToTensor(),\n",
    "        trans.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    #The transform function for validation data\n",
    "    transform_validation = trans.Compose([\n",
    "        trans.Resize((config.image_size, config.image_size)),\n",
    "        trans.ToTensor(),\n",
    "        trans.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    #The transform function for test data\n",
    "    transform_test = trans.Compose([\n",
    "        trans.Resize((config.image_size, config.image_size)),\n",
    "        trans.ToTensor(),\n",
    "        trans.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "\n",
    "    train_dataset = torchvision.datasets.ImageFolder(root = config.train_dataset_path ,transform=transform_train)\n",
    "    validation_dataset = torchvision.datasets.ImageFolder(root = config.validation_dataset_path ,transform=transform_validation)\n",
    "    evaluation_dataset = torchvision.datasets.ImageFolder(root = config.test_dataset_path ,transform=transform_test)\n",
    "\n",
    "    sampler = WRSampler(train_dataset, config.wts)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=config.train_batchsize, sampler=sampler)\n",
    "    validation_loader = torch.utils.data.DataLoader(validation_dataset, batch_size=config.eva_val_batchsize, shuffle=False)\n",
    "    evaluation_loader = torch.utils.data.DataLoader(evaluation_dataset, batch_size=config.eva_val_batchsize, shuffle=False)\n",
    "\n",
    "    if config.net == 'resnet18':\n",
    "        net = models.resnet18(pretrained=config.pretrain)\n",
    "        net.fc = nn.Sequential(nn.Linear(512,256),nn.LeakyReLU(),nn.Linear(256,128),nn.LeakyReLU(),nn.Linear(128,config.class_num))\n",
    "        net = net.to(device) \n",
    "\n",
    "    config.folder_names2code = train_dataset.class_to_idx\n",
    "    max_epoch = config.max_epoch\n",
    "    learning_rate = config.lr\n",
    "    criterion = config.criterion\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate, betas=[0.9, 0.999]) #定優化函數\n",
    "    \n",
    "    train(net, criterion, optimizer, max_epoch, train_loader, validation_loader, config)\n",
    "    #teacher_model = reload_teacher_net(config).to(device)\n",
    "    #trainKD(net, teacher_model, criterion, optimizer, max_epoch, train_loader, validation_loader, config)\n",
    "    \n",
    "    pretrain_net = reload_net(config).to(device)\n",
    "    evaluation(pretrain_net, evaluation_loader, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "Reload ./model//7.pth model.\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/weienv/lib/python3.7/site-packages/PIL/Image.py:2766: DecompressionBombWarning: Image size (91049764 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  DecompressionBombWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Top 1 Accuracy: 2999/3347 (89.60%), Top 3 Accuracy: 3347/3347 (100.00%)\n",
      "Bread                : 295/368     80.163043%\n",
      "Dairy_product        : 122/148     82.432432%\n",
      "Dessert              : 422/500     84.400000%\n",
      "Egg                  : 289/335     86.268657%\n",
      "Fried_food           : 251/287     87.456446%\n",
      "Meat                 : 407/432     94.212963%\n",
      "Noodles              : 141/147     95.918367%\n",
      "Rice                 : 88/96     91.666667%\n",
      "Seafood              : 282/303     93.069307%\n",
      "Soup                 : 480/500     96.000000%\n",
      "Vegetable_fruit      : 222/231     96.103896%\n",
      "Average per case accuracy:  89.790162%\n",
      "-----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "pretrain_net = reload_net(config).to(device)\n",
    "evaluation(pretrain_net, evaluation_loader, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
